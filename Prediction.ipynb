{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Next_Word_Prediction_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x05qpXDD0mki"
      },
      "source": [
        "# Next Word Prediction:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJKQ7gj00mkl"
      },
      "source": [
        "### Importing The Required Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMNx1i5C0mkm"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import string"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL9av8TT0mkq",
        "outputId": "3eeafe52-13f0-463a-de46-57f84c1e23b4"
      },
      "source": [
        "file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = [] # appending the words as list format\n",
        "for i in file:\n",
        "    lines.append(i) \n",
        "print(\"Starting lines of the Text: \", lines[0:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting lines of the Text:  ['\\ufeffOne morning, when Gregor Samsa woke from troubled dreams, he found\\n', 'himself transformed in his bed into a horrible vermin.  He lay on\\n', 'his armour-like back, and if he lifted his head a little he could\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmj_ah8f0mkw"
      },
      "source": [
        "### Cleaning the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "e5O4_JKy0mkx",
        "outputId": "cf2886db-a7b2-4c7e-8b18-9ad82ad738b1"
      },
      "source": [
        "data = \"\"\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "# Replacing \\n, \\r, \\ufeff with space    \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:500] # printing first 500 Words including spaces "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.  His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.  \"What\\'s happened to me?\" he'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5QythJRBbZB"
      },
      "source": [
        "# The split() method is used to split a string into an array of substrings, and returns the new array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "tSUbFpv10mk2",
        "outputId": "0bf623c8-abcd-46b3-9944-eadf58025062"
      },
      "source": [
        "z = []\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)       \n",
        "data = ' '.join(z)\n",
        "data[:500]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZaTKo8N0mk5"
      },
      "source": [
        "### Tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_Y5_GNE0mk6",
        "outputId": "439133be-1aef-472e-e4b1-bb934bcc40bb"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]  # Text is sequenced to numbers "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FqeXvA9Dl0y",
        "outputId": "e174ef9d-dec6-4368-878c-e059e824b9a2"
      },
      "source": [
        "print(tokenizer.word_index)\n",
        "total_words = len(tokenizer.word_index) + 1 # as index starts from 0 we take +1 as total words length\n",
        "print('Total Words:',total_words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'now': 1, 'gregor': 2, 'well': 3, 'it': 4, 'that': 5, 'then': 6, 'father': 7, 'he': 8, 'in': 9, 'out': 10, 'this': 11, 'so': 12, 'before': 13, 'no': 14, 'mother': 15, 'grete': 16, 'one': 17, 'samsa': 18, 'himself': 19, 'and': 20, 'him': 21, 'there': 22, 'all': 23, 'you': 24, 'yes': 25, 'again': 26, 'work': 27, 'here': 28, 'on': 29, 'like': 30, 'was': 31, 'room': 32, 'too': 33, 'be': 34, 'but': 35, 'god': 36, 'is': 37, 'soon': 38, 'enough': 39, 'would': 40, 'seven': 41, 'did': 42, 'come': 43, 'round': 44, 'door': 45, 'while': 46, 'said': 47, 'already': 48, 'will': 49, 'help': 50, 'we': 51, 'anyway': 52, 'morning': 53, 'bed': 54, 'back': 55, 'if': 56, 'little': 57, 'the': 58, 'to': 59, 'moment': 60, \"what's\": 61, 'happened': 62, 'me': 63, 'table': 64, 'had': 65, 'upright': 66, 'her': 67, 'look': 68, 'something': 69, 'do': 70, 'right': 71, 'however': 72, 'hard': 73, 'pain': 74, 'oh': 75, 'what': 76, 'business': 77, 'more': 78, 'home': 79, 'train': 80, 'time': 81, 'know': 82, 'them': 83, 'go': 84, 'up': 85, 'slowly': 86, 'better': 87, 'other': 88, 'life': 89, 'for': 90, 'gentlemen': 91, 'just': 92, 'long': 93, 'desk': 94, \"it's\": 95, 'another': 96, \"that's\": 97, 'first': 98, 'hands': 99, 'were': 100, 'later': 101, 'not': 102, \"o'clock\": 103, 'should': 104, 'through': 105, 'office': 106, 'wrong': 107, 'voice': 108, 'words': 109, 'away': 110, 'family': 111, 'sister': 112, 'sides': 113, 'breakfast': 114, 'perhaps': 115, 'today': 116, 'others': 117, 'move': 118, 'seen': 119, 'force': 120, 'way': 121, 'patient': 122, 'floor': 123, 'quiet': 124, 'left': 125, 'question': 126, \"he's\": 127, 'us': 128, 'attention': 129, 'chair': 130, 'end': 131, 'herself': 132, 'ceiling': 133, 'days': 134, 'from': 135, 'head': 136, 'see': 137, 'belly': 138, 'legs': 139, 'with': 140, 'of': 141, 'about': 142, 'as': 143, 'looked': 144, 'thought': 145, 'although': 146, 'walls': 147, 'travelling': 148, 'nice': 149, 'who': 150, 'window': 151, 'at': 152, 'heard': 153, 'how': 154, 'sleep': 155, 'longer': 156, 'because': 157, 'present': 158, 'state': 159, 'position': 160, 'where': 161, 'times': 162, 'eyes': 163, 'day': 164, 'doing': 165, 'much': 166, 'can': 167, 'friendly': 168, 'place': 169, 'quickly': 170, 'eating': 171, 'boss': 172, 'maybe': 173, 'parents': 174, 'ago': 175, 'down': 176, 'hearing': 177, 'hope': 178, 'once': 179, 'money': 180, 'years': 181, 'though': 182, 'alarm': 183, 'chest': 184, 'drawers': 185, 'half': 186, 'been': 187, 'possible': 188, 'furniture': 189, 'noise': 190, 'went': 191, 'man': 192, 'understanding': 193, 'yet': 194, 'ill': 195, 'ever': 196, 'completely': 197, 'somewhere': 198, 'wanted': 199, 'saying': 200, 'conversation': 201, 'came': 202, 'side': 203, 'word': 204, 'open': 205, 'disturbed': 206, 'dressed': 207, 'nothing': 208, 'they': 209, 'arms': 210, 'directions': 211, 'control': 212, 'body': 213, 'forward': 214, 'stay': 215, 'earlier': 216, 'confidence': 217, 'things': 218, 'carpet': 219, 'two': 220, 'use': 221, 'locked': 222, 'decision': 223, 'flat': 224, 'opened': 225, 'clerk': 226, 'leave': 227, 'say': 228, 'please': 229, 'good': 230, 'else': 231, 'evenings': 232, 'town': 233, 'evening': 234, 'kitchen': 235, \"you're\": 236, 'hand': 237, 'crying': 238, 'job': 239, 'condition': 240, 'behaviour': 241, 'am': 242, 'astonished': 243, 'late': 244, 'sir': 245, 'alright': 246, 'suffer': 247, 'strength': 248, 'wait': 249, 'listen': 250, 'tears': 251, 'realised': 252, 'listening': 253, 'key': 254, 'mouth': 255, 'wide': 256, 'movement': 257, 'disappeared': 258, 'ground': 259, 'important': 260, 'face': 261, 'until': 262, 'self': 263, 'newspaper': 264, 'hissing': 265, 'tip': 266, 'doorway': 267, 'eat': 268, 'milk': 269, 'darkness': 270, 'either': 271, 'everyone': 272, 'couch': 273, 'anxiously': 274, 'surprise': 275, 'comfortable': 276, 'asleep': 277, 'silent': 278, 'playing': 279, 'violin': 280, 'coming': 281, 'writing': 282, 'high': 283, \"let's\": 284, 'screamed': 285, 'immobile': 286, 'death': 287, 'coat': 288, 'contrary': 289, 'beards': 290, 'neck': 291, 'dead': 292, 'when': 293, 'dreams': 294, 'his': 295, 'a': 296, 'horrible': 297, 'could': 298, 'slightly': 299, 'by': 300, 'stiff': 301, 'hardly': 302, 'seemed': 303, 'ready': 304, 'off': 305, 'rest': 306, 'dream': 307, 'human': 308, 'small': 309, 'peacefully': 310, 'picture': 311, 'an': 312, 'frame': 313, 'lady': 314, 'fur': 315, 'hat': 316, 'whole': 317, 'arm': 318, 'turned': 319, 'weather': 320, 'drops': 321, 'pane': 322, 'which': 323, 'feel': 324, 'sad': 325, 'i': 326, 'get': 327, 'onto': 328, 'shut': 329, 'only': 330, 'stopped': 331, 'began': 332, 'chosen': 333, 'effort': 334, 'than': 335, 'your': 336, \"there's\": 337, 'making': 338, 'food': 339, 'different': 340, 'people': 341, 'anyone': 342, 'or': 343, 'saw': 344, \"didn't\": 345, 'make': 346, 'cold': 347, 'getting': 348, 'stupid': 349, 'salesmen': 350, 'whenever': 351, 'house': 352, 'during': 353, 'these': 354, 'still': 355, 'their': 356, 'breakfasts': 357, 'my': 358, \"i'd\": 359, 'spot': 360, 'thing': 361, 'think': 362, 'notice': 363, 'everything': 364, 'let': 365, \"he'd\": 366, 'talking': 367, 'close': 368, 'some': 369, 'together': 370, 'five': 371, \"i'll\": 372, 'big': 373, 'change': 374, 'over': 375, 'clock': 376, 'past': 377, 'quietly': 378, 'moving': 379, 'forwards': 380, 'even': 381, 'rung': 382, 'true': 383, 'probably': 384, 'next': 385, 'rush': 386, 'lively': 387, 'anger': 388, 'report': 389, 'being': 390, 'service': 391, 'doctor': 392, 'company': 393, 'son': 394, 'fact': 395, 'after': 396, 'usual': 397, 'struck': 398, 'near': 399, 'called': 400, 'whether': 401, 'answer': 402, 'circumstances': 403, 'thank': 404, \"i'm\": 405, 'outside': 406, 'explanation': 407, 'doors': 408, 'gently': 409, \"aren't\": 410, 'anything': 411, 'answered': 412, 'very': 413, 'each': 414, 'whispered': 415, 'opening': 416, 'instead': 417, 'habit': 418, 'acquired': 419, 'night': 420, 'without': 421, 'bring': 422, 'thoughts': 423, 'conclusions': 424, 'themselves': 425, 'sign': 426, 'serious': 427, 'covers': 428, 'fell': 429, 'broad': 430, 'itself': 431, 'finally': 432, 'painfully': 433, \"can't\": 434, 'done': 435, 'keep': 436, 'part': 437, 'gather': 438, 'direction': 439, 'learned': 440, 'sensitive': 441, 'turning': 442, 'easily': 443, 'despite': 444, 'weight': 445, 'eventually': 446, 'last': 447, 'injured': 448, 'afraid': 449, 'same': 450, 'price': 451, 'harder': 452, 'whatever': 453, 'sacrifice': 454, 'calm': 455, 'unfortunately': 456, 'narrow': 457, 'street': 458, 'lightly': 459, 'expected': 460, 'falling': 461, 'happen': 462, 'new': 463, 'maid': 464, 'mind': 465, 'under': 466, 'difficulty': 467, 'smile': 468, 'moved': 469, 'across': 470, 'ten': 471, 'someone': 472, 'around': 473, \"they're\": 474, 'steps': 475, 'needed': 476, 'why': 477, 'immediately': 478, 'employees': 479, 'least': 480, 'necessary': 481, 'upset': 482, 'sound': 483, 'annoyed': 484, 'boots': 485, 'speak': 486, 'mr': 487, \"isn't\": 488, 'week': 489, 'paper': 490, 'idea': 491, 'fretsaw': 492, \"you'll\": 493, 'ourselves': 494, 'stubborn': 495, 'unwell': 496, 'fortunately': 497, 'silence': 498, 'cry': 499, 'she': 500, 'danger': 501, 'suitable': 502, 'sacked': 503, 'happening': 504, 'unnecessary': 505, 'speaking': 506, 'behalf': 507, 'employer': 508, 'person': 509, 'reason': 510, 'appear': 511, 'stubbornness': 512, 'nor': 513, 'forgetting': 514, 'attack': 515, 'shocking': 516, 'insistent': 517, 'sight': 518, 'surely': 519, 'cried': 520, 'quick': 521, 'animal': 522, 'contrast': 523, \"mother's\": 524, 'anna': 525, 'hall': 526, 'girls': 527, 'situation': 528, 'drawn': 529, 'surprising': 530, 'meanwhile': 531, 'rested': 532, 'lock': 533, 'jaw': 534, 'start': 535, 'damage': 536, 'efforts': 537, 'concentration': 538, 'breath': 539, 'turn': 540, 'double': 541, 'fists': 542, 'living': 543, 'shook': 544, 'leant': 545, 'grey': 546, 'line': 547, 'piercing': 548, 'large': 549, 'meal': 550, 'reading': 551, 'newspapers': 552, 'wall': 553, 'opposite': 554, 'respect': 555, 'landing': 556, 'stairs': 557, 'below': 558, 'going': 559, 'removed': 560, 'nobody': 561, 'travellers': 562, 'enormous': 563, 'chance': 564, 'impossible': 565, 'lips': 566, 'shoulders': 567, 'secret': 568, 'foot': 569, 'panic': 570, 'stairway': 571, 'mood': 572, 'besides': 573, 'future': 574, 'women': 575, 'talk': 576, 'understood': 577, 'banister': 578, 'pleasure': 579, 'outstretched': 580, 'sake': 581, 'forgotten': 582, 'behind': 583, 'coffee': 584, 'screaming': 585, 'chin': 586, 'running': 587, 'stick': 588, 'overcoat': 589, 'frequent': 590, 'intentions': 591, 'confused': 592, 'space': 593, 'experience': 594, 'dark': 595, 'afterwards': 596, 'fully': 597, 'smell': 598, 'dish': 599, 'disappointment': 600, 'taste': 601, 'sometimes': 602, 'closed': 603, 'enter': 604, 'waited': 605, 'toe': 606, 'arrange': 607, 'empty': 608, 'shame': 609, 'nonetheless': 610, 'regret': 611, 'hunger': 612, 'opportunity': 613, 'stranger': 614, 'hungry': 615, 'old': 616, 'vegetables': 617, 'sauce': 618, 'cheese': 619, 'dry': 620, 'permanently': 621, 'liked': 622, 'injuries': 623, 'month': 624, 'foods': 625, 'stand': 626, 'startled': 627, 'breathe': 628, 'suffocating': 629, 'broom': 630, 'eaten': 631, 'midday': 632, 'indeed': 633, 'comment': 634, 'dinner': 635, 'rooms': 636, 'delay': 637, 'hour': 638, 'thanks': 639, 'despair': 640, 'earned': 641, 'unlike': 642, 'plan': 643, 'conservatory': 644, 'interrupted': 645, 'explanations': 646, 'lot': 647, 'interest': 648, 'closer': 649, 'slow': 650, 'sofa': 651, 'child': 652, 'clothes': 653, 'experienced': 654, 'lived': 655, 'middle': 656, 'easier': 657, 'transformation': 658, 'appearance': 659, 'sheet': 660, 'arrangement': 661, \"we've\": 662, 'insist': 663, 'age': 664, 'fro': 665, 'worn': 666, 'school': 667, 'changed': 668, 'glass': 669, 'bottles': 670, 'broke': 671, 'reproach': 672, 'confusion': 673, 'fainted': 674, \"she's\": 675, 'mean': 676, 'disappear': 677, 'neglected': 678, 'walk': 679, 'companions': 680, 'buttons': 681, 'collar': 682, 'cap': 683, 'bank': 684, 'pockets': 685, 'movements': 686, 'apple': 687, 'shock': 688, 'injury': 689, 'permission': 690, 'hotel': 691, 'shop': 692, 'peaceful': 693, 'ear': 694, 'smaller': 695, 'strangers': 696, 'cheek': 697, 'friends': 698, 'shown': 699, 'dust': 700, 'alone': 701, 'bedroom': 702, 'briefly': 703, 'dung': 704, 'beetle': 705, 'toward': 706, 'chairs': 707, 'corner': 708, 'play': 709, 'hurry': 710, 'gentleman': 711, 'smiled': 712, 'played': 713, 'hallway': 714, 'love': 715, 'seat': 716, 'expression': 717, 'emotion': 718, 'tenants': 719, 'action': 720, 'ahead': 721, 'brother': 722, 'certainty': 723, 'cleaner': 724, 'corpse': 725, 'letters': 726, 'irritation': 727, 'tram': 728, 'woke': 729, 'troubled': 730, 'found': 731, 'transformed': 732, 'into': 733, 'vermin': 734, 'lay': 735, 'armour': 736, 'lifted': 737, 'brown': 738, 'domed': 739, 'divided': 740, 'arches': 741, 'sections': 742, 'bedding': 743, 'able': 744, 'cover': 745, 'slide': 746, 'any': 747, 'many': 748, 'pitifully': 749, 'thin': 750, 'compared': 751, 'size': 752, 'waved': 753, 'helplessly': 754, \"wasn't\": 755, 'proper': 756, 'between': 757, 'its': 758, 'four': 759, 'familiar': 760, 'collection': 761, 'textile': 762, 'samples': 763, 'spread': 764, 'salesman': 765, 'above': 766, 'hung': 767, 'recently': 768, 'cut': 769, 'illustrated': 770, 'magazine': 771, 'housed': 772, 'gilded': 773, 'showed': 774, 'fitted': 775, 'boa': 776, 'sat': 777, 'raising': 778, 'heavy': 779, 'muff': 780, 'covered': 781, 'lower': 782, 'towards': 783, 'viewer': 784, 'dull': 785, 'rain': 786, 'hitting': 787, 'made': 788, 'quite': 789, 'bit': 790, 'forget': 791, 'nonsense': 792, 'unable': 793, 'used': 794, 'sleeping': 795, \"couldn't\": 796, 'threw': 797, 'always': 798, 'rolled': 799, 'must': 800, 'have': 801, 'tried': 802, 'hundred': 803, \"wouldn't\": 804, 'floundering': 805, 'mild': 806, 'never': 807, 'felt': 808, 'strenuous': 809, 'career': 810, \"i've\": 811, 'takes': 812, 'own': 813, 'top': 814, 'curse': 815, 'worries': 816, 'connections': 817, 'bad': 818, 'irregular': 819, 'contact': 820, 'become': 821, 'hell': 822, 'slight': 823, 'itch': 824, 'pushed': 825, 'headboard': 826, 'lift': 827, 'lots': 828, 'white': 829, 'spots': 830, 'drew': 831, 'touched': 832, 'overcome': 833, 'shudder': 834, 'slid': 835, 'former': 836, 'early': 837, 'makes': 838, \"you've\": 839, 'got': 840, 'live': 841, 'luxury': 842, 'instance': 843, 'guest': 844, 'copy': 845, 'contract': 846, 'are': 847, 'sitting': 848, 'ought': 849, 'try': 850, 'kicked': 851, 'knows': 852, 'best': 853, 'given': 854, 'gone': 855, 'told': 856, 'tell': 857, 'fall': 858, 'funny': 859, 'sort': 860, 'subordinates': 861, 'especially': 862, 'pay': 863, \"parents'\": 864, 'debt': 865, 'six': 866, 'suppose': 867, 'definitely': 868, 'leaves': 869, 'ticking': 870, 'heaven': 871, 'quarter': 872, 'set': 873, 'certainly': 874, 'rattling': 875, 'slept': 876, 'deeply': 877, 'catch': 878, 'mad': 879, 'packed': 880, 'particularly': 881, 'fresh': 882, 'avoid': 883, \"boss's\": 884, 'assistant': 885, 'put': 886, \"gregor's\": 887, 'spineless': 888, 'reported': 889, 'sick': 890, 'extremely': 891, 'strained': 892, 'suspicious': 893, 'fifteen': 894, 'medical': 895, 'insurance': 896, 'accuse': 897, 'having': 898, 'lazy': 899, 'accept': 900, \"doctor's\": 901, 'recommendation': 902, 'claim': 903, 'believed': 904, 'workshy': 905, 'entirely': 906, 'case': 907, 'apart': 908, 'excessive': 909, 'sleepiness': 910, 'hungrier': 911, 'hurriedly': 912, 'thinking': 913, 'decide': 914, 'cautious': 915, 'knock': 916, 'somebody': 917, 'want': 918, 'gentle': 919, 'shocked': 920, 'answering': 921, 'recognised': 922, 'deep': 923, 'inside': 924, 'painful': 925, 'uncontrollable': 926, 'squeaking': 927, 'mixed': 928, 'echo': 929, 'unclear': 930, 'leaving': 931, 'hearer': 932, 'unsure': 933, 'properly': 934, 'give': 935, 'full': 936, 'explain': 937, 'contented': 938, 'noticed': 939, 'wooden': 940, 'satisfied': 941, 'shuffled': 942, 'short': 943, 'members': 944, 'aware': 945, 'against': 946, 'expectations': 947, 'knocking': 948, 'fist': 949, 'warning': 950, 'deepness': 951, 'plaintively': 952, 'need': 953, 'both': 954, 'remove': 955, 'strangeness': 956, 'enunciating': 957, 'carefully': 958, 'putting': 959, 'pauses': 960, 'individual': 961, 'beg': 962, 'congratulated': 963, 'locking': 964, 'peace': 965, 'most': 966, 'consider': 967, 'sensible': 968, 'lying': 969, 'remembered': 970, 'often': 971, 'caused': 972, 'awkwardly': 973, 'pure': 974, 'imagination': 975, 'wondered': 976, 'imaginings': 977, 'resolve': 978, 'slightest': 979, 'doubt': 980, 'occupational': 981, 'hazard': 982, 'simple': 983, 'matter': 984, 'throw': 985, 'blow': 986, 'became': 987, 'difficult': 988, 'exceptionally': 989, 'push': 990, 'those': 991, 'continuously': 992, 'moreover': 993, 'bend': 994, 'stretch': 995, 'managed': 996, 'leg': 997, 'free': 998, \"don't\": 999, 'trying': 1000, 'imagine': 1001, 'almost': 1002, 'frenzy': 1003, 'carelessly': 1004, 'shoved': 1005, 'chose': 1006, 'hit': 1007, 'bedpost': 1008, 'burning': 1009, 'might': 1010, 'breadth': 1011, 'bulk': 1012, 'followed': 1013, 'air': 1014, 'occurred': 1015, 'miracle': 1016, 'carry': 1017, 'pushing': 1018, 'lose': 1019, 'consciousness': 1020, 'took': 1021, 'sighing': 1022, 'watching': 1023, 'struggled': 1024, 'bringing': 1025, 'order': 1026, 'chaos': 1027, 'remind': 1028, 'consideration': 1029, 'rushing': 1030, 'desperate': 1031, 'direct': 1032, 'clearly': 1033, 'enveloped': 1034, 'fog': 1035, 'view': 1036, 'cheer': 1037, 'offer': 1038, 'breathing': 1039, 'total': 1040, 'stillness': 1041, 'real': 1042, 'natural': 1043, 'strikes': 1044, 'ask': 1045, 'task': 1046, 'swinging': 1047, 'entire': 1048, 'length': 1049, 'succeeded': 1050, 'kept': 1051, 'raised': 1052, 'injuring': 1053, 'main': 1054, 'concern': 1055, 'loud': 1056, 'bound': 1057, 'raise': 1058, 'risked': 1059, 'sticking': 1060, 'method': 1061, 'game': 1062, 'rock': 1063, 'forth': 1064, 'strong': 1065, 'dome': 1066, 'peel': 1067, 'load': 1068, 'careful': 1069, 'swang': 1070, 'hopefully': 1071, 'find': 1072, 'really': 1073, 'call': 1074, 'suppress': 1075, 'far': 1076, 'balance': 1077, 'rocked': 1078, 'final': 1079, 'ring': 1080, \"that'll\": 1081, 'froze': 1082, 'danced': 1083, 'remained': 1084, 'caught': 1085, 'nonsensical': 1086, 'course': 1087, \"maid's\": 1088, 'firm': 1089, 'hear': 1090, \"visitor's\": 1091, 'greeting': 1092, 'knew': 1093, 'chief': 1094, 'condemned': 1095, 'highly': 1096, 'shortcoming': 1097, 'every': 1098, 'louts': 1099, 'faithful': 1100, 'devoted': 1101, 'pangs': 1102, 'conscience': 1103, 'spend': 1104, 'couple': 1105, 'hours': 1106, 'trainees': 1107, 'enquiries': 1108, 'assuming': 1109, 'show': 1110, 'innocent': 1111, 'trusted': 1112, 'wisdom': 1113, 'investigate': 1114, 'thump': 1115, 'softened': 1116, 'also': 1117, 'elastic': 1118, 'muffled': 1119, 'noticeable': 1120, 'held': 1121, 'rubbed': 1122, \"something's\": 1123, 'fallen': 1124, 'concede': 1125, 'gruff': 1126, 'reply': 1127, \"clerk's\": 1128, 'footsteps': 1129, 'polished': 1130, 'adjoining': 1131, 'daring': 1132, 'has': 1133, 'wants': 1134, 'personally': 1135, 'sure': 1136, \"he'll\": 1137, 'forgive': 1138, 'untidiness': 1139, 'continued': 1140, 'believe': 1141, 'missed': 1142, 'lad': 1143, 'thinks': 1144, 'nearly': 1145, 'cross': 1146, 'goes': 1147, 'stayed': 1148, 'sits': 1149, 'reads': 1150, 'studies': 1151, 'timetables': 1152, 'relaxation': 1153, 'working': 1154, 'three': 1155, 'amazed': 1156, 'hanging': 1157, 'opens': 1158, 'glad': 1159, 'thoughtfully': 1160, 'miss': 1161, 'explaining': 1162, 'mrs': 1163, 'commerce': 1164, 'simply': 1165, 'considerations': 1166, 'asked': 1167, 'impatiently': 1168, 'join': 1169, 'begun': 1170, 'losing': 1171, 'pursue': 1172, 'demands': 1173, 'worry': 1174, 'intention': 1175, 'abandoning': 1176, 'seriously': 1177, 'minor': 1178, 'discourtesy': 1179, 'excuse': 1180, 'disturbing': 1181, 'worried': 1182, 'barricade': 1183, 'yourself': 1184, 'causing': 1185, 'fail': 1186, 'mention': 1187, 'duties': 1188, 'unheard': 1189, 'request': 1190, 'clear': 1191, 'immediate': 1192, 'suddenly': 1193, 'seem': 1194, 'showing': 1195, 'peculiar': 1196, 'whims': 1197, 'suggest': 1198, 'failure': 1199, 'entrusted': 1200, 'giving': 1201, 'honour': 1202, 'incomprehensible': 1203, 'wish': 1204, 'whatsoever': 1205, 'intercede': 1206, 'secure': 1207, 'originally': 1208, 'intended': 1209, 'private': 1210, 'since': 1211, 'cause': 1212, 'waste': 1213, 'learn': 1214, 'turnover': 1215, 'unsatisfactory': 1216, 'grant': 1217, 'year': 1218, 'recognise': 1219, 'cannot': 1220, 'allow': 1221, 'beside': 1222, 'excitement': 1223, 'dizziness': 1224, \"haven't\": 1225, 'easy': 1226, 'symptom': 1227, 'illness': 1228, 'staying': 1229, 'basis': 1230, 'accusations': 1231, \"nobody's\": 1232, 'read': 1233, 'latest': 1234, 'contracts': 1235, 'sent': 1236, 'eight': 1237, 'few': 1238, 'recommend': 1239, 'gushed': 1240, 'knowing': 1241, 'practise': 1242, 'curious': 1243, 'responsibility': 1244, 'calmly': 1245, 'hurried': 1246, 'station': 1247, 'climb': 1248, 'smooth': 1249, 'gave': 1250, 'swing': 1251, 'stood': 1252, 'nearby': 1253, 'tightly': 1254, 'edges': 1255, 'calmed': 1256, 'understand': 1257, 'fools': 1258, \"we're\": 1259, 'communicated': 1260, 'straight': 1261, 'spoke': 1262, 'calmness': 1263, 'screams': 1264, 'entrance': 1265, 'clapping': 1266, 'locksmith': 1267, 'skirts': 1268, 'swishing': 1269, 'ran': 1270, 'wrenching': 1271, 'front': 1272, 'banging': 1273, 'homes': 1274, 'awful': 1275, 'calmer': 1276, 'clearer': 1277, 'ears': 1278, 'response': 1279, 'confident': 1280, 'wise': 1281, 'among': 1282, 'great': 1283, 'achievements': 1284, 'distinguish': 1285, 'crucial': 1286, 'coughed': 1287, 'taking': 1288, 'care': 1289, 'loudly': 1290, 'coughs': 1291, 'judge': 1292, 'whispering': 1293, 'pressed': 1294, 'holding': 1295, 'using': 1296, 'adhesive': 1297, 'tips': 1298, 'recover': 1299, 'involved': 1300, 'teeth': 1301, 'grasp': 1302, 'lack': 1303, 'ignoring': 1304, 'kind': 1305, 'fluid': 1306, 'flowed': 1307, 'dripped': 1308, 'greatly': 1309, 'encouraged': 1310, 'calling': 1311, 'hold': 1312, 'excitedly': 1313, 'following': 1314, 'paying': 1315, 'snapped': 1316, 'break': 1317, 'regained': 1318, 'handle': 1319, 'entering': 1320, 'occupied': 1321, 'exclaim': 1322, 'sounded': 1323, 'soughing': 1324, 'wind': 1325, 'nearest': 1326, 'retreating': 1327, 'driven': 1328, 'steady': 1329, 'invisible': 1330, 'hair': 1331, 'dishevelled': 1332, 'unfolded': 1333, 'sank': 1334, 'breast': 1335, 'hostile': 1336, 'clenched': 1337, 'wanting': 1338, 'uncertainly': 1339, 'wept': 1340, 'powerful': 1341, 'bolted': 1342, 'along': 1343, 'peered': 1344, 'lighter': 1345, 'endless': 1346, 'black': 1347, 'building': 1348, 'hospital': 1349, 'austere': 1350, 'regular': 1351, 'windows': 1352, 'facade': 1353, 'throwing': 1354, 'droplets': 1355, 'washing': 1356, 'several': 1357, 'number': 1358, 'exactly': 1359, 'photograph': 1360, 'lieutenant': 1361, 'army': 1362, 'sword': 1363, 'carefree': 1364, 'uniform': 1365, 'bearing': 1366, 'pack': 1367, 'commercial': 1368, 'traveller': 1369, 'arduous': 1370, 'earn': 1371, 'accurately': 1372, 'temporarily': 1373, 'remember': 1374, 'achieved': 1375, 'diligence': 1376, 'our': 1377, 'trapped': 1378, 'take': 1379, 'likes': 1380, 'wage': 1381, 'soft': 1382, 'prejudice': 1383, 'particular': 1384, 'overview': 1385, 'staff': 1386, 'businessman': 1387, 'mistakes': 1388, 'harshly': 1389, 'victim': 1390, 'gossip': 1391, 'groundless': 1392, 'complaints': 1393, 'defend': 1394, 'usually': 1395, 'arrive': 1396, 'exhausted': 1397, 'trip': 1398, 'harmful': 1399, 'effects': 1400, 'partly': 1401, 'started': 1402, 'protruding': 1403, 'stared': 1404, 'trembling': 1405, 'steadily': 1406, 'gradually': 1407, 'prohibition': 1408, 'reached': 1409, 'sudden': 1410, 'rushed': 1411, 'stretched': 1412, 'supernatural': 1413, 'waiting': 1414, 'save': 1415, 'extreme': 1416, 'convinced': 1417, 'provide': 1418, 'lost': 1419, 'won': 1420, 'depended': 1421, 'clever': 1422, 'lover': 1423, 'persuade': 1424, 'considering': 1425, 'speech': 1426, 'reach': 1427, 'ridiculously': 1428, 'scream': 1429, 'sought': 1430, 'landed': 1431, 'numerous': 1432, 'solid': 1433, 'believing': 1434, 'sorrows': 1435, 'urge': 1436, 'swayed': 1437, 'crouched': 1438, 'engrossed': 1439, 'jumped': 1440, 'fingers': 1441, 'shouting': 1442, \"pity's\": 1443, 'suggested': 1444, 'unthinking': 1445, 'hurrying': 1446, 'backwards': 1447, 'seeming': 1448, 'pot': 1449, 'knocked': 1450, 'gush': 1451, 'pouring': 1452, 'looking': 1453, 'snapping': 1454, 'jaws': 1455, 'flow': 1456, 'anew': 1457, 'fled': 1458, 'spare': 1459, 'run': 1460, 'reaching': 1461, 'leapt': 1462, 'shouts': 1463, 'resounding': 1464, 'staircase': 1465, 'flight': 1466, 'relatively': 1467, 'controlled': 1468, 'impeding': 1469, 'seized': 1470, 'picked': 1471, 'drive': 1472, 'stamping': 1473, 'appeals': 1474, 'humbly': 1475, 'merely': 1476, 'stamped': 1477, 'chilly': 1478, 'pulled': 1479, 'draught': 1480, 'flew': 1481, 'curtains': 1482, 'fluttered': 1483, 'blown': 1484, 'stop': 1485, 'drove': 1486, 'noises': 1487, 'wild': 1488, 'practice': 1489, 'allowed': 1490, 'impatient': 1491, 'threat': 1492, 'lethal': 1493, \"father's\": 1494, 'choice': 1495, 'disgust': 1496, 'incapable': 1497, 'anxious': 1498, 'glances': 1499, 'hinder': 1500, 'distance': 1501, 'unbearable': 1502, 'finished': 1503, 'mistake': 1504, 'pleased': 1505, 'further': 1506, 'obviously': 1507, 'occur': 1508, 'fixed': 1509, 'preparation': 1510, 'pleasant': 1511, 'regard': 1512, 'angle': 1513, 'flank': 1514, 'scraped': 1515, 'vile': 1516, 'flecks': 1517, 'stuck': 1518, 'fast': 1519, 'quivering': 1520, 'hefty': 1521, 'shove': 1522, 'released': 1523, 'flying': 1524, 'heavily': 1525, 'bleeding': 1526, 'slammed': 1527, 'ii': 1528, 'awoke': 1529, 'coma': 1530, 'woken': 1531, \"hadn't\": 1532, 'impression': 1533, 'leading': 1534, 'light': 1535, 'electric': 1536, 'lamps': 1537, 'shone': 1538, 'palely': 1539, 'tops': 1540, 'feeling': 1541, 'clumsily': 1542, 'antennae': 1543, 'beginning': 1544, 'value': 1545, 'scar': 1546, 'limped': 1547, 'badly': 1548, 'rows': 1549, 'events': 1550, 'dragged': 1551, 'lifelessly': 1552, 'actually': 1553, 'filled': 1554, 'sweetened': 1555, 'pieces': 1556, 'bread': 1557, 'floating': 1558, 'laughed': 1559, 'dipped': 1560, 'covering': 1561, 'tender': 1562, 'worked': 1563, 'snuffling': 1564, 'normally': 1565, 'favourite': 1566, 'drink': 1567, 'crawled': 1568, 'centre': 1569, 'crack': 1570, 'gas': 1571, 'lit': 1572, 'write': 1573, 'recent': 1574, 'lead': 1575, 'gazing': 1576, 'pride': 1577, 'such': 1578, 'wealth': 1579, 'comfort': 1580, 'frightening': 1581, 'crawling': 1582, 'resolved': 1583, 'timorous': 1584, 'visitor': 1585, 'vain': 1586, 'previous': 1587, 'unlocked': 1588, 'keys': 1589, 'gaslight': 1590, 'awake': 1591, 'distinctly': 1592, 'plenty': 1593, 'undisturbed': 1594, 're': 1595, 'tall': 1596, 'forced': 1597, 'remain': 1598, 'uneasy': 1599, 'ease': 1600, 'underneath': 1601, 'spent': 1602, 'passed': 1603, 'frequently': 1604, 'vague': 1605, 'hopes': 1606, 'led': 1607, 'conclusion': 1608, 'patience': 1609, 'greatest': 1610, 'bear': 1611, 'unpleasantness': 1612, 'impose': 1613, 'test': 1614, 'decisions': 1615, 'ended': 1616, \"god's\": 1617, 'flown': 1618, 'edge': 1619, 'watched': 1620, 'realise': 1621, 'rather': 1622, 'draw': 1623, 'terrible': 1624, \"sister's\": 1625, 'feet': 1626, 'splashed': 1627, 'rag': 1628, 'bare': 1629, 'carried': 1630, 'imagining': 1631, 'wildest': 1632, 'possibilities': 1633, 'guessed': 1634, 'goodness': 1635, 'brought': 1636, 'selection': 1637, 'rotten': 1638, 'bones': 1639, 'raisins': 1640, 'almonds': 1641, 'declared': 1642, 'inedible': 1643, 'roll': 1644, 'butter': 1645, 'salt': 1646, 'poured': 1647, 'water': 1648, 'aside': 1649, 'placed': 1650, 'feelings': 1651, 'whirred': 1652, 'healed': 1653, 'finger': 1654, 'knife': 1655, 'hurt': 1656, 'yesterday': 1657, 'less': 1658, 'sucking': 1659, 'greedily': 1660, 'compellingly': 1661, 'attracted': 1662, 'watering': 1663, 'consumed': 1664, 'lethargic': 1665, 'withdraw': 1666, 'rounded': 1667, 'bulging': 1668, 'unselfconsciously': 1669, 'swept': 1670, 'overs': 1671, 'mixing': 1672, 'dropped': 1673, 'bin': 1674, 'lid': 1675, 'received': 1676, 'second': 1677, 'send': 1678, 'errand': 1679, 'starve': 1680, 'feeding': 1681, 'distress': 1682, 'suffering': 1683, 'content': 1684, 'sighs': 1685, 'saints': 1686, 'becoming': 1687, 'construed': 1688, 'enjoyed': 1689, 'diligently': 1690, 'cleared': 1691, 'sadly': 1692, \"everything's\": 1693, 'news': 1694, 'directly': 1695, 'scurry': 1696, 'appropriate': 1697, 'press': 1698, 'seldom': 1699, 'mealtime': 1700, 'meals': 1701, 'subject': 1702, 'knees': 1703, 'begged': 1704, 'within': 1705, 'tearfully': 1706, 'thanking': 1707, 'dismissal': 1708, 'swore': 1709, 'emphatically': 1710, 'cooking': 1711, 'bother': 1712, 'ate': 1713, 'unsuccessfully': 1714, 'receive': 1715, 'similar': 1716, 'drank': 1717, 'beer': 1718, 'hoping': 1719, 'fetch': 1720, 'add': 1721, 'selfish': 1722, 'housekeeper': 1723, 'explained': 1724, 'finances': 1725, 'prospects': 1726, 'receipt': 1727, 'document': 1728, 'cash': 1729, 'box': 1730, 'saved': 1731, 'collapsed': 1732, 'complicated': 1733, 'taken': 1734, 'item': 1735, 'incarcerated': 1736, 'misfortune': 1737, 'reduced': 1738, 'fiery': 1739, 'vigour': 1740, 'junior': 1741, 'representative': 1742, 'overnight': 1743, 'ways': 1744, 'converted': 1745, 'success': 1746, 'benefit': 1747, 'delighted': 1748, 'splendour': 1749, 'costs': 1750, 'gratitude': 1751, 'warm': 1752, 'affection': 1753, 'return': 1754, 'fond': 1755, 'music': 1756, 'gifted': 1757, 'expressive': 1758, 'violinist': 1759, 'expense': 1760, 'periods': 1761, 'mentioned': 1762, 'lovely': 1763, 'decided': 1764, 'planned': 1765, 'grand': 1766, 'announcement': 1767, 'christmas': 1768, 'totally': 1769, 'pointless': 1770, 'tired': 1771, 'continue': 1772, 'wearily': 1773, 'pull': 1774, 'repeated': 1775, 'matters': 1776, 'misfortunes': 1777, 'available': 1778, 'meantime': 1779, 'accumulated': 1780, 'keeping': 1781, 'accumulating': 1782, 'nodded': 1783, 'enthusiasm': 1784, 'unexpected': 1785, 'thrift': 1786, 'caution': 1787, 'surplus': 1788, 'reduce': 1789, 'freed': 1790, 'enable': 1791, 'maintain': 1792, 'emergencies': 1793, 'healthy': 1794, 'lacking': 1795, 'holiday': 1796, 'strain': 1797, 'clumsy': 1798, 'elderly': 1799, 'suffered': 1800, 'asthma': 1801, 'struggling': 1802, 'seventeen': 1803, 'till': 1804, 'enviable': 1805, 'consisting': 1806, 'wearing': 1807, 'helping': 1808, 'joining': 1809, 'modest': 1810, 'pleasures': 1811, 'cool': 1812, 'leather': 1813, 'hot': 1814, 'lie': 1815, 'wink': 1816, 'scratching': 1817, 'climbing': 1818, 'sill': 1819, 'propped': 1820, 'leaning': 1821, 'stare': 1822, 'sense': 1823, 'freedom': 1824, 'distinct': 1825, 'known': 1826, 'charlottenstrasse': 1827, 'city': 1828, 'barren': 1829, 'sky': 1830, 'earth': 1831, 'mingled': 1832, 'inseparably': 1833, 'observant': 1834, 'twice': 1835, 'exact': 1836, 'tidied': 1837, 'inner': 1838, 'naturally': 1839, 'pretend': 1840, 'burdensome': 1841, 'unpleasant': 1842, 'entered': 1843, 'sooner': 1844, 'precaution': 1845, 'shivering': 1846, 'ordeal': 1847, 'staring': 1848, 'motionless': 1849, 'threatened': 1850, 'bite': 1851, 'hide': 1852, 'flee': 1853, 'protruded': 1854, 'carrying': 1855, 'bedsheet': 1856, 'arranged': 1857, 'bent': 1858, 'glimpsed': 1859, 'fourteen': 1860, 'appreciated': 1861, 'girl': 1862, 'somewhat': 1863, 'useless': 1864, 'behaved': 1865, 'improvement': 1866, 'visit': 1867, 'persuaded': 1868, 'listened': 1869, 'closely': 1870, 'approved': 1871, 'unfortunate': 1872, 'courage': 1873, \"adult's\": 1874, 'appreciation': 1875, 'square': 1876, 'meters': 1877, 'crawl': 1878, 'entertain': 1879, 'freely': 1880, 'relaxed': 1881, 'happy': 1882, 'letting': 1883, 'crash': 1884, 'entertaining': 1885, 'traces': 1886, 'removing': 1887, 'dare': 1888, 'sixteen': 1889, 'bravely': 1890, 'cook': 1891, 'helped': 1892, 'unless': 1893, 'choose': 1894, 'approached': 1895, 'express': 1896, 'joy': 1897, 'folds': 1898, 'thrown': 1899, 'refrained': 1900, 'spying': 1901, 'pair': 1902, 'feeble': 1903, 'heaving': 1904, 'heaviest': 1905, 'warnings': 1906, 'lasted': 1907, 'labouring': 1908, 'minutes': 1909, 'saddened': 1910, 'heart': 1911, 'abandoned': 1912, 'whose': 1913, 'whereabouts': 1914, 'tone': 1915, 'added': 1916, \"won't\": 1917, 'cope': 1918, \"it'd\": 1919, 'comes': 1920, 'unchanged': 1921, 'communication': 1922, 'monotonous': 1923, 'months': 1924, 'emptied': 1925, 'transform': 1926, 'cave': 1927, 'inherited': 1928, 'unimpeded': 1929, 'shaken': 1930, 'influence': 1931, 'mindlessly': 1932, 'loss': 1933, 'advantage': 1934, 'agree': 1935, 'spokesman': 1936, 'concerned': 1937, 'meant': 1938, 'advice': 1939, 'sufficient': 1940, 'childish': 1941, 'perversity': 1942, 'whereas': 1943, 'enthusiastic': 1944, 'tempted': 1945, 'dominated': 1946, 'refused': 1947, 'dissuade': 1948, 'groaning': 1949, 'poked': 1950, 'considerate': 1951, 'pulling': 1952, 'inch': 1953, 'startlement': 1954, 'prevent': 1955, 'attract': 1956, 'assure': 1957, 'unusual': 1958, 'admit': 1959, 'calls': 1960, 'scraping': 1961, 'assailed': 1962, 'emptying': 1963, 'dear': 1964, 'containing': 1965, 'tools': 1966, 'homework': 1967, 'trainee': 1968, 'infant': 1969, \"women's\": 1970, 'stepped': 1971, 'catching': 1972, 'sallied': 1973, 'denuded': 1974, 'copious': 1975, 'firmly': 1976, 'watch': 1977, 'shall': 1978, 'met': 1979, 'albeit': 1980, 'tremor': 1981, 'safe': 1982, 'chase': 1983, 'unyielding': 1984, 'jump': 1985, \"grete's\": 1986, 'patch': 1987, 'flowers': 1988, 'wallpaper': 1989, 'shouted': 1990, 'glowering': 1991, 'shaking': 1992, 'spoken': 1993, 'smelling': 1994, 'salts': 1995, 'faint': 1996, 'advise': 1997, 'various': 1998, 'bottle': 1999, 'splinter': 2000, 'caustic': 2001, 'medicine': 2002, 'delaying': 2003, 'oppressed': 2004, 'anxiety': 2005, 'spin': 2006, 'numb': 2007, 'arrived': 2008, 'subdued': 2009, 'openly': 2010, 'responsible': 2011, 'act': 2012, 'violence': 2013, 'subtleties': 2014, 'ah': 2015, 'sounding': 2016, 'angry': 2017, 'imagined': 2018, 'laying': 2019, 'entombed': 2020, 'trips': 2021, 'armchair': 2022, 'nightgown': 2023, 'sunday': 2024, 'public': 2025, 'wrapped': 2026, 'labour': 2027, 'walking': 2028, 'invariably': 2029, 'standing': 2030, 'smart': 2031, 'blue': 2032, 'gold': 2033, 'banking': 2034, 'institute': 2035, 'emerged': 2036, 'bushy': 2037, 'eyebrows': 2038, 'alert': 2039, 'unkempt': 2040, 'combed': 2041, 'scalp': 2042, 'monogram': 2043, 'arc': 2044, 'trouser': 2045, 'bottom': 2046, 'determination': 2047, 'walked': 2048, 'unusually': 2049, 'soles': 2050, 'wasted': 2051, 'strict': 2052, 'scurried': 2053, 'decisive': 2054, 'largely': 2055, 'feared': 2056, 'provoking': 2057, 'step': 2058, 'countless': 2059, 'noticeably': 2060, 'lungs': 2061, 'reliable': 2062, 'lurched': 2063, 'muster': 2064, 'saving': 2065, 'forgot': 2066, 'concealed': 2067, 'carved': 2068, 'notches': 2069, 'protrusions': 2070, 'tossed': 2071, 'point': 2072, 'bombard': 2073, 'fruit': 2074, 'bowl': 2075, 'sideboard': 2076, 'aim': 2077, 'red': 2078, 'apples': 2079, 'motors': 2080, 'glanced': 2081, 'harm': 2082, 'squarely': 2083, 'lodged': 2084, 'drag': 2085, 'incredible': 2086, 'changing': 2087, 'nailed': 2088, 'senses': 2089, 'blouse': 2090, 'unfastened': 2091, 'sliding': 2092, 'stumbling': 2093, 'uniting': 2094, 'ability': 2095, 'begging': 2096, 'iii': 2097, 'dared': 2098, 'flesh': 2099, 'visible': 2100, 'reminder': 2101, 'current': 2102, 'revolting': 2103, 'form': 2104, 'member': 2105, 'treated': 2106, 'enemy': 2107, 'duty': 2108, 'swallow': 2109, 'revulsion': 2110, 'mobility': 2111, 'ancient': 2112, 'invalid': 2113, 'deterioration': 2114, 'opinion': 2115, \"everyone's\": 2116, 'thus': 2117, 'differently': 2118, 'conversations': 2119, 'ones': 2120, 'longing': 2121, 'damp': 2122, 'nowadays': 2123, 'lamp': 2124, 'sew': 2125, 'fancy': 2126, 'underwear': 2127, 'fashion': 2128, 'sales': 2129, 'shorthand': 2130, 'french': 2131, 'wake': 2132, 'sewing': 2133, 'dozing': 2134, 'exchange': 2135, 'grin': 2136, 'unused': 2137, 'peg': 2138, 'slumber': 2139, 'serve': 2140, 'expecting': 2141, 'superior': 2142, 'result': 2143, 'shabbier': 2144, 'stains': 2145, 'shiny': 2146, 'uncomfortable': 2147, 'obstinate': 2148, 'regularly': 2149, 'importune': 2150, 'reproaches': 2151, 'refusing': 2152, 'tug': 2153, 'sleeve': 2154, 'whisper': 2155, 'endearments': 2156, 'effect': 2157, 'sink': 2158, 'deeper': 2159, 'abruptly': 2160, 'supported': 2161, 'needle': 2162, 'pen': 2163, 'overworked': 2164, 'absolutely': 2165, 'household': 2166, 'budget': 2167, 'dismissed': 2168, 'thick': 2169, 'boned': 2170, 'charwoman': 2171, 'flapped': 2172, 'amount': 2173, 'hoped': 2174, 'items': 2175, 'jewellery': 2176, 'belonging': 2177, 'sold': 2178, 'functions': 2179, 'celebrations': 2180, 'loudest': 2181, 'complaint': 2182, 'imaginable': 2183, 'transferring': 2184, 'address': 2185, 'reasons': 2186, 'transport': 2187, 'crate': 2188, 'holes': 2189, 'related': 2190, 'world': 2191, 'expects': 2192, 'poor': 2193, 'sacrificed': 2194, 'behest': 2195, 'customers': 2196, 'sit': 2197, 'mingle': 2198, 'eyed': 2199, \"family's\": 2200, 'affairs': 2201, 'apprentices': 2202, 'teaboy': 2203, 'businesses': 2204, 'chambermaids': 2205, 'provincial': 2206, 'memory': 2207, 'appeared': 2208, 'cashier': 2209, 'whom': 2210, 'inaccessible': 2211, 'rage': 2212, 'plans': 2213, 'pantry': 2214, 'entitled': 2215, 'sweep': 2216, 'indifferent': 2217, 'untouched': 2218, 'quicker': 2219, 'smears': 2220, 'dirt': 2221, 'balls': 2222, 'filth': 2223, 'worst': 2224, 'places': 2225, 'weeks': 2226, 'touchy': 2227, 'cleaning': 2228, 'thoroughly': 2229, 'clean': 2230, 'bucketfuls': 2231, 'dampness': 2232, 'bitter': 2233, 'punished': 2234, 'aggrieved': 2235, 'mothers': 2236, 'imploring': 2237, 'convulsive': 2238, 'helpless': 2239, 'agitated': 2240, 'accused': 2241, 'quaking': 2242, 'thumped': 2243, 'hissed': 2244, 'closing': 2245, 'widow': 2246, 'robust': 2247, 'bone': 2248, 'structure': 2249, 'withstand': 2250, 'hardest': 2251, 'repelled': 2252, 'curiosity': 2253, 'chasing': 2254, 'amazement': 2255, 'crossed': 2256, 'failed': 2257, 'considered': 2258, 'responded': 2259, 'disturb': 2260, 'windowpanes': 2261, 'indicating': 2262, 'spring': 2263, 'resentful': 2264, 'infirm': 2265, 'intending': 2266, 'prepared': 2267, 'spit': 2268, 'changes': 2269, 'anywhere': 2270, 'rented': 2271, 'earnest': 2272, 'peering': 2273, \"things'\": 2274, 'tidy': 2275, 'establishment': 2276, 'clutter': 2277, 'tolerate': 2278, 'dirty': 2279, 'furnishings': 2280, 'equipment': 2281, 'superfluous': 2282, 'discard': 2283, 'dustbins': 2284, 'chuck': 2285, 'object': 2286, 'woman': 2287, 'likely': 2288, 'junk': 2289, 'enjoy': 2290, 'lain': 2291, 'darkest': 2292, 'formerly': 2293, 'serviettes': 2294, 'knives': 2295, 'forks': 2296, 'meat': 2297, 'piled': 2298, 'potatoes': 2299, 'steaming': 2300, 'dishes': 2301, 'count': 2302, 'authority': 2303, 'piece': 2304, 'wishing': 2305, 'establish': 2306, 'sufficiently': 2307, 'cooked': 2308, 'satisfaction': 2309, 'bowed': 2310, 'mumbled': 2311, 'perfect': 2312, 'remarkable': 2313, 'chewing': 2314, 'perform': 2315, 'toothless': 2316, 'feed': 2317, 'dying': 2318, 'throughout': 2319, 'produced': 2320, 'page': 2321, 'smoking': 2322, 'attentive': 2323, 'young': 2324, 'cosy': 2325, \"we'd\": 2326, 'player': 2327, 'begin': 2328, 'therefore': 2329, 'exaggerated': 2330, 'courtesy': 2331, 'offered': 2332, 'paid': 2333, 'thoughtless': 2334, 'hidden': 2335, 'everywhere': 2336, 'threads': 2337, 'hairs': 2338, 'remains': 2339, 'wipe': 2340, 'shy': 2341, 'immaculate': 2342, 'preoccupied': 2343, 'notes': 2344, 'withdrew': 2345, 'heads': 2346, 'sunk': 2347, 'volume': 2348, 'observed': 2349, 'obvious': 2350, 'beautiful': 2351, 'disappointed': 2352, 'performance': 2353, 'politeness': 2354, 'unnerving': 2355, 'blew': 2356, 'smoke': 2357, 'cigarettes': 2358, 'upwards': 2359, 'noses': 2360, 'beautifully': 2361, 'lines': 2362, 'melancholy': 2363, 'meet': 2364, 'captivate': 2365, 'unknown': 2366, 'nourishment': 2367, 'yearning': 2368, 'determined': 2369, 'skirt': 2370, 'hiss': 2371, 'attackers': 2372, 'refuse': 2373, 'shoulder': 2374, 'kiss': 2375, 'necklace': 2376, 'pointing': 2377, 'wasting': 2378, 'forefinger': 2379, 'driving': 2380, 'attempted': 2381, 'block': 2382, 'dawning': 2383, 'realisation': 2384, 'neighbour': 2385, 'tugged': 2386, 'drop': 2387, 'bow': 2388, 'hang': 2389, 'limply': 2390, 'instrument': 2391, 'lap': 2392, 'laboriously': 2393, 'pressure': 2394, 'pillows': 2395, 'beds': 2396, 'slipped': 2397, 'obsessed': 2398, 'owed': 2399, 'urged': 2400, 'thunder': 2401, 'thereby': 2402, 'halt': 2403, 'declare': 2404, 'glancing': 2405, 'gain': 2406, 'repugnant': 2407, 'conditions': 2408, 'prevail': 2409, 'decisively': 2410, 'proceed': 2411, 'damages': 2412, 'grounds': 2413, 'joined': 2414, 'staggered': 2415, 'stretching': 2416, 'nap': 2417, 'uncontrolled': 2418, 'nodding': 2419, 'weak': 2420, 'introduction': 2421, 'monster': 2422, 'rid': 2423, 'humanly': 2424, 'cough': 2425, 'dully': 2426, 'deranged': 2427, 'forehead': 2428, 'definite': 2429, 'ideas': 2430, 'plates': 2431, 'occasionally': 2432, 'coughing': 2433, \"it'll\": 2434, 'tortured': 2435, 'endure': 2436, 'wiped': 2437, 'mechanical': 2438, 'sympathy': 2439, 'shrugged': 2440, 'helplessness': 2441, 'displacing': 2442, 'vigorously': 2443, 'acceptance': 2444, 'harmed': 2445, 'beings': 2446, 'lives': 2447, 'persecuting': 2448, 'streets': 2449, 'starting': 2450, 'beyond': 2451, 'comprehension': 2452, 'willing': 2453, 'excited': 2454, 'protect': 2455, 'startling': 2456, 'wracked': 2457, 'required': 2458, 'deal': 2459, 'repeatedly': 2460, 'striking': 2461, 'alarmed': 2462, 'unhappy': 2463, 'exhaustion': 2464, \"they'll\": 2465, 'panting': 2466, 'separated': 2467, 'noticing': 2468, 'concentrated': 2469, 'distract': 2470, 'glance': 2471, 'sprung': 2472, 'discovery': 2473, 'spindly': 2474, 'unnatural': 2475, 'aching': 2476, 'weaker': 2477, 'altogether': 2478, 'decayed': 2479, 'inflamed': 2480, 'area': 2481, 'strongly': 2482, 'rumination': 2483, 'tower': 2484, 'strike': 2485, 'weakly': 2486, 'nostrils': 2487, \"they'd\": 2488, 'slamming': 2489, \"she'd\": 2490, 'brief': 2491, 'special': 2492, 'purpose': 2493, 'martyr': 2494, 'attributed': 2495, 'tickle': 2496, 'nuisance': 2497, 'resistance': 2498, 'whistled': 2499, 'yank': 2500, 'shout': 2501, 'bedrooms': 2502, \"'ave\": 2503, 'stone': 2504, 'marriage': 2505, 'blanket': 2506, 'nightdress': 2507, 'paleness': 2508, 'confirm': 2509, 'enquiringly': 2510, 'checked': 2511, 'checking': 2512, 'replied': 2513, 'prove': 2514, 'sending': 2515, 'sideways': 2516, 'complete': 2517, 'example': 2518, 'dried': 2519, 'pained': 2520, 'warmth': 2521, 'march': 2522, 'irritably': 2523, 'men': 2524, 'coats': 2525, 'wife': 2526, 'daughter': 2527, 'disconcerted': 2528, 'sweetly': 2529, 'backs': 2530, 'continually': 2531, 'gleeful': 2532, 'anticipation': 2533, 'quarrel': 2534, 'favour': 2535, 'contents': 2536, 'rearranging': 2537, 'positions': 2538, \"we'll\": 2539, 'humility': 2540, 'strides': 2541, 'rubbing': 2542, 'friend': 2543, 'fear': 2544, 'connection': 2545, 'leader': 2546, 'hats': 2547, 'sticks': 2548, 'holder': 2549, 'premises': 2550, 'mistrust': 2551, \"men's\": 2552, 'leaned': 2553, 'progress': 2554, 'reappear': 2555, 'moments': 2556, \"butcher's\": 2557, 'boy': 2558, 'proud': 2559, 'posture': 2560, 'tray': 2561, 'nearer': 2562, 'relieved': 2563, 'wrote': 2564, 'excusal': 2565, 'employers': 2566, 'contractor': 2567, 'principal': 2568, 'tremendous': 2569, 'vertical': 2570, 'ostrich': 2571, 'feather': 2572, 'source': 2573, 'laugh': 2574, \"needn't\": 2575, 'sorted': 2576, 'intent': 2577, 'continuing': 2578, 'describing': 2579, 'detail': 2580, 'prevented': 2581, 'telling': 2582, 'peeved': 2583, 'cheerio': 2584, 'sharply': 2585, 'terribly': 2586, 'tonight': 2587, 'gets': 2588, 'destroyed': 2589, 'gained': 2590, 'twisted': 2591, 'stuff': 2592, 'kissed': 2593, 'hugged': 2594, 'country': 2595, 'sunshine': 2596, 'comfortably': 2597, 'seats': 2598, 'discussed': 2599, 'examination': 2600, 'jobs': 2601, 'promise': 2602, 'cheaper': 2603, 'location': 2604, 'practical': 2605, 'livelier': 2606, 'cheeks': 2607, 'pale': 2608, 'simultaneously': 2609, 'blossoming': 2610, 'built': 2611, 'quieter': 2612, \"other's\": 2613, 'agreed': 2614, 'confirmation': 2615, 'destination': 2616}\n",
            "Total Words: 2617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZlcq-5L0mk_",
        "outputId": "baedc176-c067-4ef5-b184-3df55c3eedbb"
      },
      "source": [
        "sequences = []\n",
        "# using Uni gram approach here \n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)  \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]\n",
        "#below it shows how unigrams is taken for each time one word is taken as Y "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Length of sequences are:  3889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 17,  53],\n",
              "       [ 53, 293],\n",
              "       [293,   2],\n",
              "       [  2,  18],\n",
              "       [ 18, 729],\n",
              "       [729, 135],\n",
              "       [135, 730],\n",
              "       [730, 294],\n",
              "       [294,   8],\n",
              "       [  8, 731]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW_YZ6yA0mlB"
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "for i in sequences:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgsb4Wy20mlD",
        "outputId": "f28b029c-0964-4403-f483-02091895be78"
      },
      "source": [
        "print(\"The Data is: \", X[:3])\n",
        "print(\"The responses are: \", y[:3])\n",
        "#Uni gram sequence looks like this"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Data is:  [ 17  53 293]\n",
            "The responses are:  [ 53 293   2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvMin9RE0mlF",
        "outputId": "74212e6e-0916-4f89-f2f9-772186d47566"
      },
      "source": [
        "# create predictors and label\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "y[:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmyGtFdt0mlH"
      },
      "source": [
        "### Creating the Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gk54qvk0mlI"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(total_words, activation=\"softmax\")) # Softmaxt used for Classification output in Neural networks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS0I4tja0mlL",
        "outputId": "7f0a3660-e37e-45cc-aa12-28e733d40dc8"
      },
      "source": [
        "#layers of the Model\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1, 10)             26170     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 1, 1000)           4044000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 1000)              8004000   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1000)              1001000   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2617)              2619617   \n",
            "=================================================================\n",
            "Total params: 15,694,787\n",
            "Trainable params: 15,694,787\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH6OaFtu0mlQ"
      },
      "source": [
        "### Callbacks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKHFhKG00mlQ"
      },
      "source": [
        "#dump the model as the runtime time of the model is taking time dumped model will be used for predictions further\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "logdir='logsnextword1'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgtGrMR50mlS"
      },
      "source": [
        "### Compile The Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxZeNJap0mlT"
      },
      "source": [
        "# Compiling the model using Categorical mesureas and checling the metric as accuracy\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X55av5l0mlU"
      },
      "source": [
        "### Fit The Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zJKKn_v0mlV",
        "outputId": "5b506f42-22fd-4d14-ded4-c6040131cfe8"
      },
      "source": [
        "# fitting the model for 150 epochs as it got some optimization according to the trained data\r\n",
        "history = model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            " 1/61 [..............................] - ETA: 0s - loss: 7.8698 - accuracy: 0.0000e+00WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8754 - accuracy: 0.0015\n",
            "Epoch 00001: loss improved from inf to 7.87541, saving model to nextword2.h5\n",
            "61/61 [==============================] - 17s 286ms/step - loss: 7.8754 - accuracy: 0.0015\n",
            "Epoch 2/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8623 - accuracy: 0.0044\n",
            "Epoch 00002: loss improved from 7.87541 to 7.86233, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 7.8623 - accuracy: 0.0044\n",
            "Epoch 3/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8231 - accuracy: 0.0039\n",
            "Epoch 00003: loss improved from 7.86233 to 7.82311, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 7.8231 - accuracy: 0.0039\n",
            "Epoch 4/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.6756 - accuracy: 0.0041\n",
            "Epoch 00004: loss improved from 7.82311 to 7.67556, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 7.6756 - accuracy: 0.0041\n",
            "Epoch 5/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.4779 - accuracy: 0.0036\n",
            "Epoch 00005: loss improved from 7.67556 to 7.47791, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 238ms/step - loss: 7.4779 - accuracy: 0.0036\n",
            "Epoch 6/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.2924 - accuracy: 0.0028\n",
            "Epoch 00006: loss improved from 7.47791 to 7.29245, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 245ms/step - loss: 7.2924 - accuracy: 0.0028\n",
            "Epoch 7/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.1291 - accuracy: 0.0028\n",
            "Epoch 00007: loss improved from 7.29245 to 7.12908, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 7.1291 - accuracy: 0.0028\n",
            "Epoch 8/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.9102 - accuracy: 0.0049\n",
            "Epoch 00008: loss improved from 7.12908 to 6.91023, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 6.9102 - accuracy: 0.0049\n",
            "Epoch 9/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.6293 - accuracy: 0.0039\n",
            "Epoch 00009: loss improved from 6.91023 to 6.62929, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 6.6293 - accuracy: 0.0039\n",
            "Epoch 10/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.3803 - accuracy: 0.0049\n",
            "Epoch 00010: loss improved from 6.62929 to 6.38026, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 6.3803 - accuracy: 0.0049\n",
            "Epoch 11/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.1614 - accuracy: 0.0046\n",
            "Epoch 00011: loss improved from 6.38026 to 6.16136, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 6.1614 - accuracy: 0.0046\n",
            "Epoch 12/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.9580 - accuracy: 0.0098\n",
            "Epoch 00012: loss improved from 6.16136 to 5.95801, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 5.9580 - accuracy: 0.0098\n",
            "Epoch 13/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.7665 - accuracy: 0.0113\n",
            "Epoch 00013: loss improved from 5.95801 to 5.76655, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 5.7665 - accuracy: 0.0113\n",
            "Epoch 14/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.5930 - accuracy: 0.0154\n",
            "Epoch 00014: loss improved from 5.76655 to 5.59303, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 5.5930 - accuracy: 0.0154\n",
            "Epoch 15/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.4137 - accuracy: 0.0211\n",
            "Epoch 00015: loss improved from 5.59303 to 5.41369, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 234ms/step - loss: 5.4137 - accuracy: 0.0211\n",
            "Epoch 16/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.2665 - accuracy: 0.0206\n",
            "Epoch 00016: loss improved from 5.41369 to 5.26652, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 5.2665 - accuracy: 0.0206\n",
            "Epoch 17/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.1240 - accuracy: 0.0242\n",
            "Epoch 00017: loss improved from 5.26652 to 5.12402, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 5.1240 - accuracy: 0.0242\n",
            "Epoch 18/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.9980 - accuracy: 0.0321\n",
            "Epoch 00018: loss improved from 5.12402 to 4.99799, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 4.9980 - accuracy: 0.0321\n",
            "Epoch 19/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.8588 - accuracy: 0.0303\n",
            "Epoch 00019: loss improved from 4.99799 to 4.85884, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 4.8588 - accuracy: 0.0303\n",
            "Epoch 20/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.7867 - accuracy: 0.0337\n",
            "Epoch 00020: loss improved from 4.85884 to 4.78672, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 242ms/step - loss: 4.7867 - accuracy: 0.0337\n",
            "Epoch 21/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6806 - accuracy: 0.0378\n",
            "Epoch 00021: loss improved from 4.78672 to 4.68060, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 4.6806 - accuracy: 0.0378\n",
            "Epoch 22/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5469 - accuracy: 0.0465\n",
            "Epoch 00022: loss improved from 4.68060 to 4.54686, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 4.5469 - accuracy: 0.0465\n",
            "Epoch 23/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4467 - accuracy: 0.0453\n",
            "Epoch 00023: loss improved from 4.54686 to 4.44669, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 4.4467 - accuracy: 0.0453\n",
            "Epoch 24/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3357 - accuracy: 0.0550\n",
            "Epoch 00024: loss improved from 4.44669 to 4.33573, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 4.3357 - accuracy: 0.0550\n",
            "Epoch 25/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2166 - accuracy: 0.0635\n",
            "Epoch 00025: loss improved from 4.33573 to 4.21664, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 4.2166 - accuracy: 0.0635\n",
            "Epoch 26/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0757 - accuracy: 0.0728\n",
            "Epoch 00026: loss improved from 4.21664 to 4.07566, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 4.0757 - accuracy: 0.0728\n",
            "Epoch 27/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9211 - accuracy: 0.0748\n",
            "Epoch 00027: loss improved from 4.07566 to 3.92115, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 3.9211 - accuracy: 0.0748\n",
            "Epoch 28/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8156 - accuracy: 0.0967\n",
            "Epoch 00028: loss improved from 3.92115 to 3.81557, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 250ms/step - loss: 3.8156 - accuracy: 0.0967\n",
            "Epoch 29/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.6856 - accuracy: 0.0995\n",
            "Epoch 00029: loss improved from 3.81557 to 3.68562, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 3.6856 - accuracy: 0.0995\n",
            "Epoch 30/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.5439 - accuracy: 0.1142\n",
            "Epoch 00030: loss improved from 3.68562 to 3.54386, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 3.5439 - accuracy: 0.1142\n",
            "Epoch 31/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.4291 - accuracy: 0.1250\n",
            "Epoch 00031: loss improved from 3.54386 to 3.42908, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 3.4291 - accuracy: 0.1250\n",
            "Epoch 32/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.2974 - accuracy: 0.1358\n",
            "Epoch 00032: loss improved from 3.42908 to 3.29739, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 3.2974 - accuracy: 0.1358\n",
            "Epoch 33/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.1807 - accuracy: 0.1620\n",
            "Epoch 00033: loss improved from 3.29739 to 3.18073, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 234ms/step - loss: 3.1807 - accuracy: 0.1620\n",
            "Epoch 34/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.1208 - accuracy: 0.1623\n",
            "Epoch 00034: loss improved from 3.18073 to 3.12079, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 3.1208 - accuracy: 0.1623\n",
            "Epoch 35/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.0143 - accuracy: 0.1736\n",
            "Epoch 00035: loss improved from 3.12079 to 3.01430, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 3.0143 - accuracy: 0.1736\n",
            "Epoch 36/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8991 - accuracy: 0.1936\n",
            "Epoch 00036: loss improved from 3.01430 to 2.89907, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 2.8991 - accuracy: 0.1936\n",
            "Epoch 37/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8526 - accuracy: 0.2001\n",
            "Epoch 00037: loss improved from 2.89907 to 2.85260, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 242ms/step - loss: 2.8526 - accuracy: 0.2001\n",
            "Epoch 38/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7830 - accuracy: 0.2121\n",
            "Epoch 00038: loss improved from 2.85260 to 2.78296, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 2.7830 - accuracy: 0.2121\n",
            "Epoch 39/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7118 - accuracy: 0.2193\n",
            "Epoch 00039: loss improved from 2.78296 to 2.71183, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 2.7118 - accuracy: 0.2193\n",
            "Epoch 40/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6460 - accuracy: 0.2265\n",
            "Epoch 00040: loss improved from 2.71183 to 2.64598, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 2.6460 - accuracy: 0.2265\n",
            "Epoch 41/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5582 - accuracy: 0.2451\n",
            "Epoch 00041: loss improved from 2.64598 to 2.55823, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 2.5582 - accuracy: 0.2451\n",
            "Epoch 42/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4911 - accuracy: 0.2566\n",
            "Epoch 00042: loss improved from 2.55823 to 2.49113, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 238ms/step - loss: 2.4911 - accuracy: 0.2566\n",
            "Epoch 43/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4824 - accuracy: 0.2571\n",
            "Epoch 00043: loss improved from 2.49113 to 2.48236, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 2.4824 - accuracy: 0.2571\n",
            "Epoch 44/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4327 - accuracy: 0.2597\n",
            "Epoch 00044: loss improved from 2.48236 to 2.43266, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 2.4327 - accuracy: 0.2597\n",
            "Epoch 45/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3652 - accuracy: 0.2728\n",
            "Epoch 00045: loss improved from 2.43266 to 2.36523, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 2.3652 - accuracy: 0.2728\n",
            "Epoch 46/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3347 - accuracy: 0.2723\n",
            "Epoch 00046: loss improved from 2.36523 to 2.33474, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 2.3347 - accuracy: 0.2723\n",
            "Epoch 47/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2518 - accuracy: 0.2924\n",
            "Epoch 00047: loss improved from 2.33474 to 2.25184, saving model to nextword2.h5\n",
            "61/61 [==============================] - 16s 262ms/step - loss: 2.2518 - accuracy: 0.2924\n",
            "Epoch 48/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2238 - accuracy: 0.3006\n",
            "Epoch 00048: loss improved from 2.25184 to 2.22379, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 2.2238 - accuracy: 0.3006\n",
            "Epoch 49/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1925 - accuracy: 0.3062\n",
            "Epoch 00049: loss improved from 2.22379 to 2.19246, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 2.1925 - accuracy: 0.3062\n",
            "Epoch 50/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1785 - accuracy: 0.3106\n",
            "Epoch 00050: loss improved from 2.19246 to 2.17855, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 2.1785 - accuracy: 0.3106\n",
            "Epoch 51/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1263 - accuracy: 0.3183\n",
            "Epoch 00051: loss improved from 2.17855 to 2.12630, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 2.1263 - accuracy: 0.3183\n",
            "Epoch 52/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0705 - accuracy: 0.3327\n",
            "Epoch 00052: loss improved from 2.12630 to 2.07054, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 234ms/step - loss: 2.0705 - accuracy: 0.3327\n",
            "Epoch 53/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0508 - accuracy: 0.3332\n",
            "Epoch 00053: loss improved from 2.07054 to 2.05079, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 2.0508 - accuracy: 0.3332\n",
            "Epoch 54/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0266 - accuracy: 0.3397\n",
            "Epoch 00054: loss improved from 2.05079 to 2.02658, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 234ms/step - loss: 2.0266 - accuracy: 0.3397\n",
            "Epoch 55/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0113 - accuracy: 0.3379\n",
            "Epoch 00055: loss improved from 2.02658 to 2.01131, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 234ms/step - loss: 2.0113 - accuracy: 0.3379\n",
            "Epoch 56/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9883 - accuracy: 0.3458\n",
            "Epoch 00056: loss improved from 2.01131 to 1.98832, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 233ms/step - loss: 1.9883 - accuracy: 0.3458\n",
            "Epoch 57/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9501 - accuracy: 0.3543\n",
            "Epoch 00057: loss improved from 1.98832 to 1.95005, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 1.9501 - accuracy: 0.3543\n",
            "Epoch 58/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9212 - accuracy: 0.3538\n",
            "Epoch 00058: loss improved from 1.95005 to 1.92115, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 234ms/step - loss: 1.9212 - accuracy: 0.3538\n",
            "Epoch 59/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9052 - accuracy: 0.3572\n",
            "Epoch 00059: loss improved from 1.92115 to 1.90523, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 1.9052 - accuracy: 0.3572\n",
            "Epoch 60/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8639 - accuracy: 0.3718\n",
            "Epoch 00060: loss improved from 1.90523 to 1.86392, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 1.8639 - accuracy: 0.3718\n",
            "Epoch 61/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8407 - accuracy: 0.3811\n",
            "Epoch 00061: loss improved from 1.86392 to 1.84072, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 1.8407 - accuracy: 0.3811\n",
            "Epoch 62/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7694 - accuracy: 0.3988\n",
            "Epoch 00062: loss improved from 1.84072 to 1.76937, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 234ms/step - loss: 1.7694 - accuracy: 0.3988\n",
            "Epoch 63/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7473 - accuracy: 0.4016\n",
            "Epoch 00063: loss improved from 1.76937 to 1.74725, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 234ms/step - loss: 1.7473 - accuracy: 0.4016\n",
            "Epoch 64/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7335 - accuracy: 0.4078\n",
            "Epoch 00064: loss improved from 1.74725 to 1.73350, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 1.7335 - accuracy: 0.4078\n",
            "Epoch 65/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7318 - accuracy: 0.4122\n",
            "Epoch 00065: loss improved from 1.73350 to 1.73177, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 1.7318 - accuracy: 0.4122\n",
            "Epoch 66/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7204 - accuracy: 0.3983\n",
            "Epoch 00066: loss improved from 1.73177 to 1.72040, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 1.7204 - accuracy: 0.3983\n",
            "Epoch 67/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7411 - accuracy: 0.4016\n",
            "Epoch 00067: loss did not improve from 1.72040\n",
            "61/61 [==============================] - 14s 228ms/step - loss: 1.7411 - accuracy: 0.4016\n",
            "Epoch 68/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7140 - accuracy: 0.4127\n",
            "Epoch 00068: loss improved from 1.72040 to 1.71405, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 234ms/step - loss: 1.7140 - accuracy: 0.4127\n",
            "Epoch 69/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6905 - accuracy: 0.4060\n",
            "Epoch 00069: loss improved from 1.71405 to 1.69052, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 1.6905 - accuracy: 0.4060\n",
            "Epoch 70/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6646 - accuracy: 0.4171\n",
            "Epoch 00070: loss improved from 1.69052 to 1.66456, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 1.6646 - accuracy: 0.4171\n",
            "Epoch 71/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6434 - accuracy: 0.4232\n",
            "Epoch 00071: loss improved from 1.66456 to 1.64337, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 1.6434 - accuracy: 0.4232\n",
            "Epoch 72/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6295 - accuracy: 0.4281\n",
            "Epoch 00072: loss improved from 1.64337 to 1.62954, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 243ms/step - loss: 1.6295 - accuracy: 0.4281\n",
            "Epoch 73/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5860 - accuracy: 0.4389\n",
            "Epoch 00073: loss improved from 1.62954 to 1.58604, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 1.5860 - accuracy: 0.4389\n",
            "Epoch 74/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5480 - accuracy: 0.4500\n",
            "Epoch 00074: loss improved from 1.58604 to 1.54798, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 1.5480 - accuracy: 0.4500\n",
            "Epoch 75/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5384 - accuracy: 0.4466\n",
            "Epoch 00075: loss improved from 1.54798 to 1.53842, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 234ms/step - loss: 1.5384 - accuracy: 0.4466\n",
            "Epoch 76/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5528 - accuracy: 0.4433\n",
            "Epoch 00076: loss did not improve from 1.53842\n",
            "61/61 [==============================] - 14s 228ms/step - loss: 1.5528 - accuracy: 0.4433\n",
            "Epoch 77/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5389 - accuracy: 0.4420\n",
            "Epoch 00077: loss did not improve from 1.53842\n",
            "61/61 [==============================] - 14s 230ms/step - loss: 1.5389 - accuracy: 0.4420\n",
            "Epoch 78/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5260 - accuracy: 0.4451\n",
            "Epoch 00078: loss improved from 1.53842 to 1.52601, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 1.5260 - accuracy: 0.4451\n",
            "Epoch 79/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4972 - accuracy: 0.4513\n",
            "Epoch 00079: loss improved from 1.52601 to 1.49720, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 1.4972 - accuracy: 0.4513\n",
            "Epoch 80/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4670 - accuracy: 0.4608\n",
            "Epoch 00080: loss improved from 1.49720 to 1.46696, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 1.4670 - accuracy: 0.4608\n",
            "Epoch 81/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4654 - accuracy: 0.4724\n",
            "Epoch 00081: loss improved from 1.46696 to 1.46539, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 238ms/step - loss: 1.4654 - accuracy: 0.4724\n",
            "Epoch 82/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4548 - accuracy: 0.4708\n",
            "Epoch 00082: loss improved from 1.46539 to 1.45480, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 1.4548 - accuracy: 0.4708\n",
            "Epoch 83/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4753 - accuracy: 0.4634\n",
            "Epoch 00083: loss did not improve from 1.45480\n",
            "61/61 [==============================] - 14s 231ms/step - loss: 1.4753 - accuracy: 0.4634\n",
            "Epoch 84/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4593 - accuracy: 0.4680\n",
            "Epoch 00084: loss did not improve from 1.45480\n",
            "61/61 [==============================] - 14s 231ms/step - loss: 1.4593 - accuracy: 0.4680\n",
            "Epoch 85/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4402 - accuracy: 0.4644\n",
            "Epoch 00085: loss improved from 1.45480 to 1.44022, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 1.4402 - accuracy: 0.4644\n",
            "Epoch 86/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4382 - accuracy: 0.4729\n",
            "Epoch 00086: loss improved from 1.44022 to 1.43819, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 1.4382 - accuracy: 0.4729\n",
            "Epoch 87/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4050 - accuracy: 0.4785\n",
            "Epoch 00087: loss improved from 1.43819 to 1.40501, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 1.4050 - accuracy: 0.4785\n",
            "Epoch 88/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3990 - accuracy: 0.4873\n",
            "Epoch 00088: loss improved from 1.40501 to 1.39902, saving model to nextword2.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 1.3990 - accuracy: 0.4873\n",
            "Epoch 89/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3742 - accuracy: 0.4865\n",
            "Epoch 00089: loss improved from 1.39902 to 1.37417, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 1.3742 - accuracy: 0.4865\n",
            "Epoch 90/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3482 - accuracy: 0.4955\n",
            "Epoch 00090: loss improved from 1.37417 to 1.34823, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 1.3482 - accuracy: 0.4955\n",
            "Epoch 91/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3369 - accuracy: 0.4950\n",
            "Epoch 00091: loss improved from 1.34823 to 1.33693, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 1.3369 - accuracy: 0.4950\n",
            "Epoch 92/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3325 - accuracy: 0.4896\n",
            "Epoch 00092: loss improved from 1.33693 to 1.33248, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 245ms/step - loss: 1.3325 - accuracy: 0.4896\n",
            "Epoch 93/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3200 - accuracy: 0.4983\n",
            "Epoch 00093: loss improved from 1.33248 to 1.31996, saving model to nextword2.h5\n",
            "61/61 [==============================] - 18s 287ms/step - loss: 1.3200 - accuracy: 0.4983\n",
            "Epoch 94/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2839 - accuracy: 0.5096\n",
            "Epoch 00094: loss improved from 1.31996 to 1.28391, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 245ms/step - loss: 1.2839 - accuracy: 0.5096\n",
            "Epoch 95/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2595 - accuracy: 0.5179\n",
            "Epoch 00095: loss improved from 1.28391 to 1.25947, saving model to nextword2.h5\n",
            "61/61 [==============================] - 16s 268ms/step - loss: 1.2595 - accuracy: 0.5179\n",
            "Epoch 96/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2627 - accuracy: 0.5132\n",
            "Epoch 00096: loss did not improve from 1.25947\n",
            "61/61 [==============================] - 14s 233ms/step - loss: 1.2627 - accuracy: 0.5132\n",
            "Epoch 97/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2558 - accuracy: 0.5217\n",
            "Epoch 00097: loss improved from 1.25947 to 1.25578, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 1.2558 - accuracy: 0.5217\n",
            "Epoch 98/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2273 - accuracy: 0.5199\n",
            "Epoch 00098: loss improved from 1.25578 to 1.22732, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 1.2273 - accuracy: 0.5199\n",
            "Epoch 99/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2112 - accuracy: 0.5330\n",
            "Epoch 00099: loss improved from 1.22732 to 1.21119, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 1.2112 - accuracy: 0.5330\n",
            "Epoch 100/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2016 - accuracy: 0.5246\n",
            "Epoch 00100: loss improved from 1.21119 to 1.20160, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 1.2016 - accuracy: 0.5246\n",
            "Epoch 101/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2029 - accuracy: 0.5269\n",
            "Epoch 00101: loss did not improve from 1.20160\n",
            "61/61 [==============================] - 14s 233ms/step - loss: 1.2029 - accuracy: 0.5269\n",
            "Epoch 102/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2200 - accuracy: 0.5207\n",
            "Epoch 00102: loss did not improve from 1.20160\n",
            "61/61 [==============================] - 14s 233ms/step - loss: 1.2200 - accuracy: 0.5207\n",
            "Epoch 103/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2167 - accuracy: 0.5207\n",
            "Epoch 00103: loss did not improve from 1.20160\n",
            "\n",
            "Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "61/61 [==============================] - 14s 233ms/step - loss: 1.2167 - accuracy: 0.5207\n",
            "Epoch 104/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9186 - accuracy: 0.6138\n",
            "Epoch 00104: loss improved from 1.20160 to 0.91865, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 0.9186 - accuracy: 0.6138\n",
            "Epoch 105/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8070 - accuracy: 0.6184\n",
            "Epoch 00105: loss improved from 0.91865 to 0.80699, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 0.8070 - accuracy: 0.6184\n",
            "Epoch 106/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7716 - accuracy: 0.6053\n",
            "Epoch 00106: loss improved from 0.80699 to 0.77155, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 243ms/step - loss: 0.7716 - accuracy: 0.6053\n",
            "Epoch 107/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7518 - accuracy: 0.5948\n",
            "Epoch 00107: loss improved from 0.77155 to 0.75181, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 0.7518 - accuracy: 0.5948\n",
            "Epoch 108/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7419 - accuracy: 0.5914\n",
            "Epoch 00108: loss improved from 0.75181 to 0.74191, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 242ms/step - loss: 0.7419 - accuracy: 0.5914\n",
            "Epoch 109/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7341 - accuracy: 0.5904\n",
            "Epoch 00109: loss improved from 0.74191 to 0.73405, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 246ms/step - loss: 0.7341 - accuracy: 0.5904\n",
            "Epoch 110/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7302 - accuracy: 0.5863\n",
            "Epoch 00110: loss improved from 0.73405 to 0.73025, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 0.7302 - accuracy: 0.5863\n",
            "Epoch 111/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7266 - accuracy: 0.5888\n",
            "Epoch 00111: loss improved from 0.73025 to 0.72662, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 0.7266 - accuracy: 0.5888\n",
            "Epoch 112/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7239 - accuracy: 0.5816\n",
            "Epoch 00112: loss improved from 0.72662 to 0.72388, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 236ms/step - loss: 0.7239 - accuracy: 0.5816\n",
            "Epoch 113/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7210 - accuracy: 0.5806\n",
            "Epoch 00113: loss improved from 0.72388 to 0.72102, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 245ms/step - loss: 0.7210 - accuracy: 0.5806\n",
            "Epoch 114/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7218 - accuracy: 0.5845\n",
            "Epoch 00114: loss did not improve from 0.72102\n",
            "61/61 [==============================] - 15s 248ms/step - loss: 0.7218 - accuracy: 0.5845\n",
            "Epoch 115/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7213 - accuracy: 0.5798\n",
            "Epoch 00115: loss did not improve from 0.72102\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 0.7213 - accuracy: 0.5798\n",
            "Epoch 116/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7172 - accuracy: 0.5775\n",
            "Epoch 00116: loss improved from 0.72102 to 0.71718, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 0.7172 - accuracy: 0.5775\n",
            "Epoch 117/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7161 - accuracy: 0.5847\n",
            "Epoch 00117: loss improved from 0.71718 to 0.71608, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 238ms/step - loss: 0.7161 - accuracy: 0.5847\n",
            "Epoch 118/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7158 - accuracy: 0.5778\n",
            "Epoch 00118: loss improved from 0.71608 to 0.71582, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 0.7158 - accuracy: 0.5778\n",
            "Epoch 119/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7157 - accuracy: 0.5780\n",
            "Epoch 00119: loss improved from 0.71582 to 0.71573, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 0.7157 - accuracy: 0.5780\n",
            "Epoch 120/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7123 - accuracy: 0.5780\n",
            "Epoch 00120: loss improved from 0.71573 to 0.71229, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 0.7123 - accuracy: 0.5780\n",
            "Epoch 121/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7123 - accuracy: 0.5770\n",
            "Epoch 00121: loss improved from 0.71229 to 0.71227, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 243ms/step - loss: 0.7123 - accuracy: 0.5770\n",
            "Epoch 122/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.5801\n",
            "Epoch 00122: loss improved from 0.71227 to 0.71164, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 0.7116 - accuracy: 0.5801\n",
            "Epoch 123/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7113 - accuracy: 0.5811\n",
            "Epoch 00123: loss improved from 0.71164 to 0.71135, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 242ms/step - loss: 0.7113 - accuracy: 0.5811\n",
            "Epoch 124/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7088 - accuracy: 0.5822\n",
            "Epoch 00124: loss improved from 0.71135 to 0.70879, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 0.7088 - accuracy: 0.5822\n",
            "Epoch 125/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7090 - accuracy: 0.5788\n",
            "Epoch 00125: loss did not improve from 0.70879\n",
            "61/61 [==============================] - 14s 233ms/step - loss: 0.7090 - accuracy: 0.5788\n",
            "Epoch 126/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7075 - accuracy: 0.5773\n",
            "Epoch 00126: loss improved from 0.70879 to 0.70753, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 243ms/step - loss: 0.7075 - accuracy: 0.5773\n",
            "Epoch 127/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7102 - accuracy: 0.5793\n",
            "Epoch 00127: loss did not improve from 0.70753\n",
            "61/61 [==============================] - 14s 233ms/step - loss: 0.7102 - accuracy: 0.5793\n",
            "Epoch 128/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7081 - accuracy: 0.5822\n",
            "Epoch 00128: loss did not improve from 0.70753\n",
            "61/61 [==============================] - 14s 228ms/step - loss: 0.7081 - accuracy: 0.5822\n",
            "Epoch 129/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7071 - accuracy: 0.5762\n",
            "Epoch 00129: loss improved from 0.70753 to 0.70711, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 0.7071 - accuracy: 0.5762\n",
            "Epoch 130/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7048 - accuracy: 0.5819\n",
            "Epoch 00130: loss improved from 0.70711 to 0.70479, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 0.7048 - accuracy: 0.5819\n",
            "Epoch 131/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7042 - accuracy: 0.5834\n",
            "Epoch 00131: loss improved from 0.70479 to 0.70417, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 0.7042 - accuracy: 0.5834\n",
            "Epoch 132/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7034 - accuracy: 0.5791\n",
            "Epoch 00132: loss improved from 0.70417 to 0.70345, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 238ms/step - loss: 0.7034 - accuracy: 0.5791\n",
            "Epoch 133/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7040 - accuracy: 0.5719\n",
            "Epoch 00133: loss did not improve from 0.70345\n",
            "61/61 [==============================] - 14s 230ms/step - loss: 0.7040 - accuracy: 0.5719\n",
            "Epoch 134/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.5765\n",
            "Epoch 00134: loss did not improve from 0.70345\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 0.7037 - accuracy: 0.5765\n",
            "Epoch 135/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7028 - accuracy: 0.5827\n",
            "Epoch 00135: loss improved from 0.70345 to 0.70284, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 0.7028 - accuracy: 0.5827\n",
            "Epoch 136/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7033 - accuracy: 0.5834\n",
            "Epoch 00136: loss did not improve from 0.70284\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 0.7033 - accuracy: 0.5834\n",
            "Epoch 137/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7035 - accuracy: 0.5811\n",
            "Epoch 00137: loss did not improve from 0.70284\n",
            "61/61 [==============================] - 14s 231ms/step - loss: 0.7035 - accuracy: 0.5811\n",
            "Epoch 138/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.5775\n",
            "Epoch 00138: loss improved from 0.70284 to 0.70084, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 0.7008 - accuracy: 0.5775\n",
            "Epoch 139/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.5765\n",
            "Epoch 00139: loss improved from 0.70084 to 0.70068, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 0.7007 - accuracy: 0.5765\n",
            "Epoch 140/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6972 - accuracy: 0.5798\n",
            "Epoch 00140: loss improved from 0.70068 to 0.69725, saving model to nextword2.h5\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 0.6972 - accuracy: 0.5798\n",
            "Epoch 141/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6985 - accuracy: 0.5773\n",
            "Epoch 00141: loss did not improve from 0.69725\n",
            "61/61 [==============================] - 14s 232ms/step - loss: 0.6985 - accuracy: 0.5773\n",
            "Epoch 142/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6969 - accuracy: 0.5755\n",
            "Epoch 00142: loss improved from 0.69725 to 0.69694, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 243ms/step - loss: 0.6969 - accuracy: 0.5755\n",
            "Epoch 143/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.5770\n",
            "Epoch 00143: loss improved from 0.69694 to 0.69581, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 0.6958 - accuracy: 0.5770\n",
            "Epoch 144/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.5765\n",
            "Epoch 00144: loss did not improve from 0.69581\n",
            "61/61 [==============================] - 14s 235ms/step - loss: 0.6967 - accuracy: 0.5765\n",
            "Epoch 145/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.5858\n",
            "Epoch 00145: loss improved from 0.69581 to 0.69445, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 0.6944 - accuracy: 0.5858\n",
            "Epoch 146/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6962 - accuracy: 0.5809\n",
            "Epoch 00146: loss did not improve from 0.69445\n",
            "61/61 [==============================] - 14s 232ms/step - loss: 0.6962 - accuracy: 0.5809\n",
            "Epoch 147/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.5762\n",
            "Epoch 00147: loss did not improve from 0.69445\n",
            "61/61 [==============================] - 14s 231ms/step - loss: 0.6945 - accuracy: 0.5762\n",
            "Epoch 148/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6950 - accuracy: 0.5775\n",
            "Epoch 00148: loss did not improve from 0.69445\n",
            "\n",
            "Epoch 00148: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "61/61 [==============================] - 14s 229ms/step - loss: 0.6950 - accuracy: 0.5775\n",
            "Epoch 149/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6452 - accuracy: 0.5917\n",
            "Epoch 00149: loss improved from 0.69445 to 0.64516, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 0.6452 - accuracy: 0.5917\n",
            "Epoch 150/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6447 - accuracy: 0.5860\n",
            "Epoch 00150: loss improved from 0.64516 to 0.64472, saving model to nextword2.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 0.6447 - accuracy: 0.5860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBhHXvQciuH5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irZF00rJDczG"
      },
      "source": [
        "## Accuracy of the model for each epoch \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "zkmey3JVivKZ",
        "outputId": "bf3dcf7d-e7bc-49cd-ab73-1e3354268923"
      },
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}